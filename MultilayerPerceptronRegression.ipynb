{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW1OLDP0DLUauYy9bgxBwg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_TensorFlow/blob/main/MultilayerPerceptronRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. MLP model (same structure as your code)\n",
        "# ---------------------------------------------------------\n",
        "class MLP(tf.keras.Model):\n",
        "    def __init__(self, layer_sizes):\n",
        "        super().__init__()\n",
        "        self.weights_list = []\n",
        "        self.biases_list = []\n",
        "\n",
        "        for in_dim, out_dim in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
        "            W = self.add_weight(\n",
        "                shape=(in_dim, out_dim),\n",
        "                initializer=tf.keras.initializers.HeNormal(),\n",
        "                trainable=True\n",
        "            )\n",
        "            b = self.add_weight(\n",
        "                shape=(out_dim,),\n",
        "                initializer=\"zeros\",\n",
        "                trainable=True\n",
        "            )\n",
        "            self.weights_list.append(W)\n",
        "            self.biases_list.append(b)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = x\n",
        "        for W, b in zip(self.weights_list[:-1], self.biases_list[:-1]):\n",
        "            out = tf.nn.relu(tf.matmul(out, W) + b)\n",
        "        output = tf.matmul(out, self.weights_list[-1]) + self.biases_list[-1]\n",
        "        return output\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Create synthetic regression dataset\n",
        "# ---------------------------------------------------------\n",
        "np.random.seed(42)\n",
        "X = np.random.uniform(-2, 2, (2000, 1)).astype(\"float32\")\n",
        "y = (3 * X[:, 0]**2 + 2 * X[:, 0] + 1 + 0.3*np.random.randn(2000)).astype(\"float32\")\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test = X[:1500], X[1500:]\n",
        "y_train, y_test = y[:1500], y[1500:]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Build regression MLP\n",
        "# ---------------------------------------------------------\n",
        "model = MLP([1, 64, 64, 1])   # input=1, output=1\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. Compile for regression\n",
        "# ---------------------------------------------------------\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=\"mse\",\n",
        "    metrics=[\"mae\"]\n",
        ")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. Train\n",
        "# ---------------------------------------------------------\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6. Evaluate\n",
        "# ---------------------------------------------------------\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test MSE: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 7. Predict example\n",
        "# ---------------------------------------------------------\n",
        "x_new = tf.constant([[1.5]], dtype=tf.float32)\n",
        "print(\"Prediction for x=1.5:\", model(x_new).numpy())"
      ],
      "metadata": {
        "id": "XP56Xrac0csc",
        "outputId": "ef2fd062-321d-4ab5-d3ff-4541bfb49f92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 50.5078 - mae: 5.5690\n",
            "Epoch 2/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7795 - mae: 1.0170\n",
            "Epoch 3/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2311 - mae: 0.9387\n",
            "Epoch 4/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2233 - mae: 0.9519\n",
            "Epoch 5/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2066 - mae: 0.9474\n",
            "Epoch 6/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1355 - mae: 0.9035\n",
            "Epoch 7/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0956 - mae: 0.8929\n",
            "Epoch 8/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0774 - mae: 0.8889\n",
            "Epoch 9/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9630 - mae: 0.8223\n",
            "Epoch 10/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8671 - mae: 0.7739\n",
            "Epoch 11/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8317 - mae: 0.7520\n",
            "Epoch 12/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7665 - mae: 0.7131\n",
            "Epoch 13/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7394 - mae: 0.7074\n",
            "Epoch 14/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6718 - mae: 0.6671\n",
            "Epoch 15/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6082 - mae: 0.6362\n",
            "Epoch 16/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5760 - mae: 0.6060\n",
            "Epoch 17/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5046 - mae: 0.5674\n",
            "Epoch 18/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4427 - mae: 0.5293\n",
            "Epoch 19/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4009 - mae: 0.4953\n",
            "Epoch 20/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3657 - mae: 0.4785\n",
            "Epoch 21/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3194 - mae: 0.4483\n",
            "Epoch 22/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3315 - mae: 0.4517\n",
            "Epoch 23/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2937 - mae: 0.4237\n",
            "Epoch 24/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2489 - mae: 0.3947\n",
            "Epoch 25/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2370 - mae: 0.3778\n",
            "Epoch 26/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2281 - mae: 0.3688\n",
            "Epoch 27/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2008 - mae: 0.3560\n",
            "Epoch 28/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1908 - mae: 0.3465\n",
            "Epoch 29/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1794 - mae: 0.3377\n",
            "Epoch 30/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1710 - mae: 0.3247\n",
            "Epoch 31/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1513 - mae: 0.3046\n",
            "Epoch 32/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1442 - mae: 0.2996\n",
            "Epoch 33/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1474 - mae: 0.3022\n",
            "Epoch 34/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1424 - mae: 0.2939\n",
            "Epoch 35/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1468 - mae: 0.3020\n",
            "Epoch 36/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1358 - mae: 0.2872\n",
            "Epoch 37/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1284 - mae: 0.2838\n",
            "Epoch 38/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1167 - mae: 0.2703\n",
            "Epoch 39/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1189 - mae: 0.2699\n",
            "Epoch 40/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1138 - mae: 0.2670\n",
            "Epoch 41/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1148 - mae: 0.2678\n",
            "Epoch 42/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1115 - mae: 0.2657\n",
            "Epoch 43/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1091 - mae: 0.2596\n",
            "Epoch 44/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1042 - mae: 0.2581\n",
            "Epoch 45/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1127 - mae: 0.2663\n",
            "Epoch 46/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1045 - mae: 0.2562\n",
            "Epoch 47/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1062 - mae: 0.2571\n",
            "Epoch 48/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0986 - mae: 0.2461\n",
            "Epoch 49/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1030 - mae: 0.2510\n",
            "Epoch 50/50\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1067 - mae: 0.2572\n",
            "Test MSE: 0.1092, Test MAE: 0.2665\n",
            "Prediction for x=1.5: [[11.006559]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Example synthetic regression data\n",
        "# -------------------------------------------------------\n",
        "# X has 10 features, y is a continuous target\n",
        "X = np.random.randn(1000, 10).astype(\"float32\")\n",
        "y = (X @ np.arange(1, 11) * 0.5 + 3).astype(\"float32\")  # simple linear-ish target\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Build MLP model using Sequential\n",
        "# -------------------------------------------------------\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(10,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)  # regression → no activation\n",
        "])\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Compile model\n",
        "# -------------------------------------------------------\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Train model\n",
        "# -------------------------------------------------------\n",
        "history = model.fit(\n",
        "    X, y,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# Predict\n",
        "# -------------------------------------------------------\n",
        "pred = model.predict(X[:5])\n",
        "print(\"Predictions:\", pred)"
      ],
      "metadata": {
        "id": "aZQ_KLOT061T",
        "outputId": "739b2739-00ac-4591-86bc-707d75fd27b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 89.3808 - mae: 7.5493 - val_loss: 77.3967 - val_mae: 7.2514\n",
            "Epoch 2/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 77.6548 - mae: 7.0081 - val_loss: 57.5153 - val_mae: 6.2213\n",
            "Epoch 3/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 51.1737 - mae: 5.5727 - val_loss: 27.0493 - val_mae: 4.1601\n",
            "Epoch 4/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 19.4106 - mae: 3.3048 - val_loss: 4.0576 - val_mae: 1.4663\n",
            "Epoch 5/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.0475 - mae: 1.2359 - val_loss: 1.6323 - val_mae: 1.0029\n",
            "Epoch 6/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.4872 - mae: 0.9801 - val_loss: 0.9814 - val_mae: 0.7710\n",
            "Epoch 7/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.9516 - mae: 0.7726 - val_loss: 0.7452 - val_mae: 0.6703\n",
            "Epoch 8/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7788 - mae: 0.6974 - val_loss: 0.5727 - val_mae: 0.5917\n",
            "Epoch 9/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6745 - mae: 0.6420 - val_loss: 0.4655 - val_mae: 0.5361\n",
            "Epoch 10/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5149 - mae: 0.5568 - val_loss: 0.3861 - val_mae: 0.4800\n",
            "Epoch 11/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4198 - mae: 0.5061 - val_loss: 0.3236 - val_mae: 0.4408\n",
            "Epoch 12/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3628 - mae: 0.4710 - val_loss: 0.2761 - val_mae: 0.4069\n",
            "Epoch 13/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3135 - mae: 0.4356 - val_loss: 0.2428 - val_mae: 0.3789\n",
            "Epoch 14/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2820 - mae: 0.4170 - val_loss: 0.2244 - val_mae: 0.3649\n",
            "Epoch 15/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2556 - mae: 0.4013 - val_loss: 0.1975 - val_mae: 0.3329\n",
            "Epoch 16/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2532 - mae: 0.3893 - val_loss: 0.1823 - val_mae: 0.3211\n",
            "Epoch 17/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2110 - mae: 0.3604 - val_loss: 0.1696 - val_mae: 0.3104\n",
            "Epoch 18/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1739 - mae: 0.3273 - val_loss: 0.1582 - val_mae: 0.3020\n",
            "Epoch 19/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1728 - mae: 0.3234 - val_loss: 0.1536 - val_mae: 0.2982\n",
            "Epoch 20/20\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.1656 - mae: 0.3173 - val_loss: 0.1434 - val_mae: 0.2870\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "Predictions: [[-3.2240164 ]\n",
            " [17.978264  ]\n",
            " [ 7.2385325 ]\n",
            " [ 6.569209  ]\n",
            " [ 0.55721176]]\n"
          ]
        }
      ]
    }
  ]
}