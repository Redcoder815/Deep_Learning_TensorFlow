{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_TensorFlow/blob/main/08LayersAndModules.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "RpdDtbm6oB12"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10),\n",
        "])\n",
        "\n",
        "X = tf.random.uniform((2, 20))\n",
        "net(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RWorWoDoTUV",
        "outputId": "b37074dc-bd7b-4f2a-bb33-44af3f1cde6e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Module"
      ],
      "metadata": {
        "id": "o4Vo5PKGqKeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        # Call the constructor of the parent class tf.keras.Model to perform\n",
        "        # the necessary initialization\n",
        "        super().__init__()\n",
        "        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
        "        self.out = tf.keras.layers.Dense(units=10)\n",
        "\n",
        "    # Define the forward propagation of the model, that is, how to return the\n",
        "    # required model output based on the input X\n",
        "    def call(self, X):\n",
        "        return self.out(self.hidden((X)))"
      ],
      "metadata": {
        "id": "R0xpMhzypRl_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = MLP()\n",
        "net(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRtMu40XpVyH",
        "outputId": "2a5e236b-95bc-4598-daa8-8a48deb504e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential Module"
      ],
      "metadata": {
        "id": "QLjCSQwGqP5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MySequential(tf.keras.Model):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        self.modules = args\n",
        "\n",
        "    def call(self, X):\n",
        "        for module in self.modules:\n",
        "            X = module(X)\n",
        "        return X"
      ],
      "metadata": {
        "id": "dzval8wspuHM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = MySequential(\n",
        "    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(10))\n",
        "net(X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib5Y-NbLp3Qx",
        "outputId": "2f779900-8384-43b1-f3bf-0403da8acb76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward propagation method"
      ],
      "metadata": {
        "id": "YcK4ckEVqXb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedHiddenMLP(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        # Random weight parameters created with tf.constant are not updated\n",
        "        # during training (i.e., constant parameters)\n",
        "        self.rand_weight = tf.constant(tf.random.uniform((20, 20)))\n",
        "        self.dense = tf.keras.layers.Dense(20, activation=tf.nn.relu)\n",
        "\n",
        "    @tf.function # Add this decorator\n",
        "    def call(self, inputs):\n",
        "        X = self.flatten(inputs)\n",
        "        # Use the created constant parameters, as well as the relu and\n",
        "        # matmul functions\n",
        "        X = tf.nn.relu(tf.matmul(X, self.rand_weight) + 1)\n",
        "        # Reuse the fully connected layer. This is equivalent to sharing\n",
        "        # parameters with two fully connected layers\n",
        "        X = self.dense(X)\n",
        "        # Control flow\n",
        "        while tf.reduce_sum(tf.math.abs(X)) > 1:\n",
        "            X /= 2\n",
        "        return tf.reduce_sum(X)"
      ],
      "metadata": {
        "id": "Zac5jSFIqdP1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = FixedHiddenMLP()\n",
        "net(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVeFlVqCrCjr",
        "outputId": "0a23ee79-5fc3-45cc-984b-4747e97b90be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.8112651109695435>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chimera = tf.keras.Sequential(): This line initializes an empty tf.keras.Sequential model. A Sequential model is a linear stack of layers, where data flows from one layer to the next in order.\n",
        "\n",
        "chimera.add(NestMLP()): The NestMLP model (which we just discussed) is added as the first layer to chimera. This means that when data is fed into chimera, it will first pass through the entire NestMLP structure.\n",
        "\n",
        "chimera.add(tf.keras.layers.Dense(20)): Next, a standard tf.keras.layers.Dense (fully connected) layer with 20 units is added. The output from the NestMLP layer will then be fed into this dense layer.\n",
        "\n",
        "chimera.add(FixedHiddenMLP()): Finally, the FixedHiddenMLP model is added as the last layer to chimera. The output from the Dense(20) layer will be passed through the FixedHiddenMLP model.\n",
        "\n",
        "chimera(X): This line executes the entire chimera sequential model by passing the input tensor X through all the added layers in the order they were added. The output will be the result of FixedHiddenMLP processing the output of the Dense(20) layer, which in turn processed the output of NestMLP, which initially took X as input."
      ],
      "metadata": {
        "id": "as14iJ1kuYAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NestMLP(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = tf.keras.Sequential()\n",
        "        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))\n",
        "        self.dense = tf.keras.layers.Dense(16, activation=tf.nn.relu)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense(self.net(inputs))\n",
        "\n",
        "chimera = tf.keras.Sequential()\n",
        "chimera.add(NestMLP())\n",
        "chimera.add(tf.keras.layers.Dense(20))\n",
        "chimera.add(FixedHiddenMLP())\n",
        "chimera(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa9AvFyxr11e",
        "outputId": "3147d957-c1e8-453d-fcc3-590480fbb0af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.8148218393325806>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}