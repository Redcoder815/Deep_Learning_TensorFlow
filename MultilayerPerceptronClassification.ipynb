{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLnrfRQQoXGRWPkmoWJz/o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redcoder815/Deep_Learning_TensorFlow/blob/main/MultilayerPerceptronClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# 1. Create dummy dataset\n",
        "# -------------------------\n",
        "# num_samples = 1000\n",
        "# num_features = 32\n",
        "# num_classes = 10\n",
        "\n",
        "# X = np.random.randn(num_samples, num_features).astype(\"float32\")\n",
        "# y = np.random.randint(0, num_classes, size=(num_samples,))\n",
        "\n",
        "# Load MNIST\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize to 0–1\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Flatten 28x28 → 784\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# -------------------------\n",
        "# 2. Build your MLP model\n",
        "# -------------------------\n",
        "class MLP(tf.keras.Model):\n",
        "    def __init__(self, layer_size):\n",
        "        super().__init__()\n",
        "        self.weights_list = []\n",
        "        self.bias_list = []\n",
        "\n",
        "        for in_dim, out_dim in zip(layer_size[:-1], layer_size[1:]):\n",
        "            w = self.add_weight(\n",
        "                shape=(in_dim, out_dim),\n",
        "                initializer=tf.keras.initializers.HeNormal(),\n",
        "                trainable=True\n",
        "            )\n",
        "            b = self.add_weight(\n",
        "                shape=(out_dim,),\n",
        "                initializer=\"zeros\",\n",
        "                trainable=True\n",
        "            )\n",
        "            self.weights_list.append(w)\n",
        "            self.bias_list.append(b)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = x\n",
        "        for w, b in zip(self.weights_list[:-1], self.bias_list[:-1]):\n",
        "            # out = tf.nn.relu(tf.matmul(out, w) + b)\n",
        "            out = tf.nn.gelu(tf.matmul(out, w) + b)\n",
        "\n",
        "        logits = tf.matmul(out, self.weights_list[-1]) + self.bias_list[-1]\n",
        "        #for multiclass classification.\n",
        "        return tf.nn.softmax(logits)\n",
        "        # for binary classification\n",
        "        # return tf.nn.sigmoid(logits)\n",
        "\n",
        "# -------------------------\n",
        "# 3. Instantiate model\n",
        "# -------------------------\n",
        "# Corrected: The first element of layer_size must match the input features (784 for MNIST)\n",
        "model = MLP([784, 512, 512, 256, 128, 64, 10])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 4. Train model\n",
        "# -------------------------\n",
        "# Corrected: Use x_train and y_train for training\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "# -------------------------\n",
        "# 5. Evaluate model\n",
        "# -------------------------\n",
        "# Corrected: Use x_test and y_test for evaluation\n",
        "loss, acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test accuracy:\", acc)\n",
        "\n",
        "# -------------------------\n",
        "# 6. Make predictions\n",
        "# -------------------------\n",
        "sample = x_train[:5]\n",
        "pred = model(sample)\n",
        "print(\"Predicted class probabilities:\\n\", pred.numpy())\n",
        "print(\"Predicted classes:\", tf.argmax(pred, axis=1).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQPuMz0DoX84",
        "outputId": "e65ed3b5-74d5-4850-805d-d98b24c3ab79"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 17ms/step - accuracy: 0.8981 - loss: 0.3362\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 17ms/step - accuracy: 0.9714 - loss: 0.0942\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.9802 - loss: 0.0669\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.9855 - loss: 0.0489\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.9887 - loss: 0.0374\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9752 - loss: 0.0916\n",
            "Test accuracy: 0.9796000123023987\n",
            "Predicted class probabilities:\n",
            " [[9.31491861e-09 1.35876085e-11 1.91381147e-11 3.51412200e-05\n",
            "  2.31152857e-11 9.99962926e-01 1.71912788e-07 8.94537500e-11\n",
            "  4.12665258e-07 1.26185989e-06]\n",
            " [1.00000000e+00 8.64347060e-13 6.42081319e-11 5.83289597e-11\n",
            "  1.90970243e-12 1.75774256e-10 1.74547647e-08 2.87640842e-13\n",
            "  6.78307688e-11 2.24842284e-10]\n",
            " [6.79571022e-09 4.57912774e-09 9.84930239e-07 4.68095944e-11\n",
            "  9.99977231e-01 8.50517934e-10 5.91384328e-08 8.44218473e-09\n",
            "  2.37898572e-08 2.16886056e-05]\n",
            " [8.63462600e-12 1.00000000e+00 1.08332072e-10 2.33556020e-14\n",
            "  3.67424217e-13 8.01450139e-12 4.68207883e-13 6.70702773e-12\n",
            "  2.57777666e-09 3.84177977e-15]\n",
            " [3.37850352e-08 3.21473237e-07 6.96012776e-08 3.07829328e-06\n",
            "  3.35625700e-05 5.61242416e-07 2.73800382e-09 4.10532266e-05\n",
            "  3.13646495e-07 9.99920964e-01]]\n",
            "Predicted classes: [5 0 4 1 9]\n"
          ]
        }
      ]
    }
  ]
}